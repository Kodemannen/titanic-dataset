# -*- coding: utf-8 -*-
"""titanic_data_cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OtcncWQW-RFcwNzRH4i4-As6Oj-JU71p
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_recall_fscore_support
import os

import matplotlib.pyplot as plt

import seaborn as sns
sns.set()

this_directory = os.path.dirname(os.path.realpath(__file__))


"""# Extract, transform, and load
Extract, transform, and load (ETL) is an approach used to retrieve data from different sources, transform them according to certain requirements and then load the transformed data into a desired source. This notebook contains a short walkthrough of an ETL approach, with an additional focus on inspecting, contemplating and transforming the raw data.

Our raw Titanic dataset is here loaded into a panda in python, from a CSV file. Below that, we will import a dataset with abbreviations for relevant ports.
"""

#titanic_dataset = pd.read_csv(this_directory+"\\titanic.csv")
titanic_dataset = pd.read_csv(os.path.join(this_directory, "titanic.csv"))

#print(titanic_dataset.head())



"""Koden under printer de fem første radene i datasettet. Har du noen tanker om datasettet fra det du ser? Forklaring på noen av kolonnenavnene:

Pclass - Passenger class (fare class)

Sibsp - Amount of siblings and potential partner onboard	

Parch -  Amount of parents or children onboard	

Ticket - ticket number	

Fare - ticket price	

Cabin - cabin number	

Embarked - port of embarkment  
"""





def plot_distributions():
    fig, axes = plt.subplots(nrows=4, ncols=2, sharex=False,  sharey=False)

    fig.set_size_inches([8.3, 11.7])  # A4

    #----------------------------------------------------------------------------------------------
    # Age distribution:
    #----------------------------------------------------------------------------------------------
    ages = titanic_dataset["Age"]
    ax = axes[0,0]
    ax.hist(ages, bins=np.arange(ages.max()), label="Ages")
    ax.set_xlabel("age (year)")
    ax.set_ylabel("N individuals")
    ax.legend()




    #----------------------------------------------------------------------------------------------
    # Ticket price distribution:
    #----------------------------------------------------------------------------------------------
    fares = titanic_dataset["Fare"]
    ax = axes[0,1]
    fares.hist(ax=ax, label="Ticket prices")
    # ax.hist(fares, bins = np.arange(fares.max(), step=10), label="Ticket price")
    # ax.set_xlabel("price (pound)")
    # ax.set_ylabel("N tickets")
    ax.legend()


    #----------------------------------------------------------------------------------------------
    # Survivorship distribution:
    #----------------------------------------------------------------------------------------------
    survivorship = titanic_dataset["Survived"]
    ax = axes[1,0]
    survivorship.hist(ax=ax, rwidth=1, bins=[0,1,2], label="Survivorship")
    ax.set_xticks([0.5, 1.49])
    ax.set_ylabel("N individuals")
    ax.legend()
    #ax.hist(survivorship, bins=[0,1,2], label="Survivorship")
    # ax.set_xlabel("")
    # ax.set_xticks([0.5, 1.5])
    # ax.set_xticklabels(["died", "survived"])
    ax.set_ylabel("N individuals")
    ax.set_xticklabels(["died", "survived"])
    ax.legend()


    #----------------------------------------------------------------------------------------------
    # Sex distribution:
    #----------------------------------------------------------------------------------------------
    sex = titanic_dataset["Sex"]

    ax = axes[1,1]
    hist = sex.hist(ax=ax, rwidth=1, bins=[0,1,2], label="Sex")
    ax.set_xticks([0.5, 1.49])
    ax.set_ylabel("N individuals")
    ax.legend()
    #ax.set_xticklabels(["", "Survived"])


    #----------------------------------------------------------------------------------------------
    # Parch distribution:
    #----------------------------------------------------------------------------------------------
    parch = titanic_dataset["Parch"]
    print(parch.max()) 

    ax = axes[2,0]
    hist = parch.hist(ax=ax, bins=np.arange(parch.max()+2)-0.5, label="Parch (children)")
    ax.set_xticks([0,1,2,3,4,5,6])
    #ax.semilogy()
    ax.set_ylabel("N individuals")
    ax.set_xlabel("N children")
    ax.legend()


    #----------------------------------------------------------------------------------------------
    # Passenger class distribution:
    #----------------------------------------------------------------------------------------------
    passenger_class = titanic_dataset["Pclass"]

    ax = axes[2,1]
    hist = passenger_class.hist(ax=ax, 
                      bins=np.arange(0,4)+0.5, 
                      label="Passenger classes")

    ax.set_xticks([1,2,3])
    # ax.set_ylabel("N individuals")
    ax.set_xlabel("class")
    ax.legend()


    #----------------------------------------------------------------------------------------------
    # Sibling/spouse distribution:
    #----------------------------------------------------------------------------------------------
    sibspouse = titanic_dataset["SibSp"]

    ax = axes[3,0]
    sibspouse.hist(ax=ax, 
                   bins=np.arange(sibspouse.max()+2)-0.5, 
                   label="Siblings/spouses")

    #ax.set_xticks([0.5, 1.49])
    ax.set_ylabel("N individuals")
    ax.set_xlabel("N siblings/spouses")
    ax.legend()


    #----------------------------------------------------------------------------------------------
    # Embarked distribution:
    #----------------------------------------------------------------------------------------------
    data = titanic_dataset["Embarked"]

    ax = axes[3,1]
    data.hist(ax=ax, 
               #bins=np.arange(data.max()+2)-0.5, 
               #bins=[0, 1,2,3],
               label="Embarked")

    #ax.set_xticks([0.5,1.5,2.5])
    ax.set_ylabel("N individuals")
    ax.set_xlabel("port")
    ax.legend()

    fig.suptitle("Parameter distributions", )
    # padding title: (not suptitle)
    fig.tight_layout(rect=[0, 0.03, 1, 0.95])
    #ttl.set_position([.5,1.0])
    fig.savefig("figs/distributions.pdf")






#pd.set_option('max_columns', 12)





"""### How to interpret this dataset?
What is your first thoughts about this dataset? Composition of different data types, possible analytical utilizations, lack of information for your possible utilization, quality etc.?

In addition, we have a dataset containing the full port name and the linked abbreviation for each port. Run the code in order to load and print the dataset. 

The columns are:

Abbrevation - abbreviation linked to 'Embarked' in titanic.csv (foreign key)

Port_name - full port name
"""


"""
Tanker om datasettet:
    * Tanker:
        - Stor variasjon i pris
        - Overlevelse og kjønn har ganske like histogrammer 

    * Se på sannsynlighet for overlevelse som funksjon av billettpris
    * Se på sannsynlighet for overlevelse som funksjon av alder
    * Se på sannsynlighet for overlevelse som funksjon av kjønn

    * Are we counting parch twice if both parents are on the ship and counted?

"""


#ports_dataset = pd.read_csv(this_directory+"\\port_abbrevations.csv")
ports_dataset = pd.read_csv(os.path.join(this_directory, "port_abbrevations.csv"))
ports_dataset.head()



"""## Transform
Before the dataset should be utilized, some steps remain to be undertaken. In the upcoming steps, you will be asked to inspect, clean and enhance the quality of the dataset. Please think about other steps that should be consideren in this phase. If such exist, are they nessecary, are they neglectable etc.?

### Handle empty entries (missing values)
Run the following code to print Null values for each column. What do you see? Which actions can be taken when missing values appear in your dataset?
"""

"""
"""

# Detecting missing values:

for col in titanic_dataset.columns:
    pct_missing = np.mean(titanic_dataset[col].isnull())
    print('{} - {}%'.format(col, round(pct_missing*100,1)))

"""
Please develop your code in the appropriate slots to perform the following transformations:
- Remove the column "Cabin".
- Replace Null values in the "Age" column with average age.
- Remove passengers with Null values in the "Embarked" column."""



# Code here to remove the "Cabin" column

# Code here to replace Null values in the "Age" column with the average age

# Code here to remove passengers with Null values in the "Embarked" column (Passengers without embarkment information)

"""### Handle duplicate data entries
Why is it unfavorable with duplicates in our dataset? Inspect if there duplicates exist in the titanic.csv dataset. 
"""

# Code here to inspect if duplicates exist in titanic.csv (titanic_dataset)

"""### Relevant and nessecary information distributed among different sources
A particular dataset will often lack essential informationm. Information is often distributed among a wide range of databases, applications, files etc. If relevant data is distributed between two tables, as in our case, and we need to gather the information, what could be a way forward?
"""

# Code here to add port name to each entry (passenger) in the titanic_dataset

"""### Join two datasets
In our case, we would like to have the full port name added to every entry (passenger) in our titantic dataset.
"""

# Code here to add port name to each entry (passenger) in the titanic_dataset

"""### Irrelevant columns
Picture us utilizing the dataset for passenger survival predictions. Are there columns that should be removed or that you consider fairly irrelevant for such a prediction?
"""

# Run the following to inspect the present columns
titanic_dataset.columns

# Hint: Does some columns reflect the same information? If so, remove them from the dataset.
# Code here to remove columns if deemed nessecary

"""## Save
We need to save our new enhanced dataset.
"""

# Code here to save titanic_dataset as a new CSV

"""### Additional questions
Have you made up your mind regarding other transformations, inspections or measures that should be taken to increase the quality and consistency of the titanic dataset?
"""

# Write some thoughts around the bonus question here:

"""# Simple Survival Modelling

Create a model that predicts if a given passenger survives, based on columns you deem relevant. You can choose an approach or a technique yourself. The accuracy and applicability is not too important, but we would like you to make some considerations around the approach (is it actually suitable, what is the advantages or disadvantages etc.)
"""

# Implement a modell predicting 'Survived' here. 
# Tip: It may be that the data set needs some further transformation. 
# Tips: Remember that you can easily import a library or method of your choice.

"""To obtain an impression of your model's performance, we would like to have a quick look at the precision and recall. Please obtain the precision and recall for your survival prediciton model, and comment on the findings. 

"""

# Obtain the recall and precision here.



def main():
    plot_distributions()
    return 0

if __name__=="__main__":
    main()
